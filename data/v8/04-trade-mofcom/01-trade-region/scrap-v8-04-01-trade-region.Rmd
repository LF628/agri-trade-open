---
title: "中国商务部农产品贸易专题网站抓取"
author: "胡华平"
date: "`r Sys.Date()`"
output:
  bookdown::word_document2:
    fig_caption: yes
    toc: yes
    toc_depth: 4
    reference_docx: report-reference.docx
  word_document:
    toc: no
    toc_depth: '4'
  bookdown::html_document2:
    number_sections: yes
    toc: no
    fig_caption: yes
    toc_float: yes
always_allow_html: yes
---

```{r, echo=FALSE}
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE,
                      fig.align='center',fig.width=10, fig.height=7) # Places figures on their own pages
options(
  htmltools.dir.version = FALSE, 
  formatR.indent = 2, width = 55, 
  digits = 2,scipen=999,tinytex.verbose = TRUE,
  knitr.kable.NA = '',
  fig.width=12, fig.height=8)

```

```{r}
require("rvest")
require("stringr")
require("tidyverse")
require("tidyselect")
```

\newpage

# 1.研究目标

目标是把中国海关发布的两张表数据（表15和表16）——部分国家(地区)出口/进口商品类章金额表（人民币值）——所有数据整理出来。初步分析，我国主要农产品进/出口国家和地区的趋势和关系。

这个目标比较明确和具体，数据是可获得的。数据集最终为中国跟各个国家在主要农产品上的月度（20余年x12月/年=`r 20*12+5`）进出口贸易额。

# 2.数据集

## 2.1资料来源1：商务部网站

中国商务部-[农产品贸易专题网站](http://wms.mofcom.gov.cn/article/ztxx/ncpmy/)

**《中国农产品进出口月度统计报告》**:

- 文件标题："中国农产品进出口月度统计报告2020年1月"
- 可下载年限范围：2002/01-2020/02
- 数据频率：月度
- 文件格式：pdf-制式表格

**《重要农产品出口月度报告》**:

- 文件标题：
    - ["中国农产品出口月度统计报告-大米"](http://wms.mofcom.gov.cn/aarticle/Nocategory/200609/20060903269241.html)
    - ["中国农产品出口月度统计报告-小麦"](http://wms.mofcom.gov.cn/aarticle/Nocategory/200609/20060903269081.html)
    - [中国农产品出口月度统计报告-玉米](http://wms.mofcom.gov.cn/aarticle/Nocategory/200609/20060903269063.html)
- 可下载年限范围：2006/12-2020/03
- 数据频率：月度
- 文件格式：pdf-制式表格


## 2.2资料来源2：搜数网-中国农业统计年鉴


搜数网[人大权限](https://libproxy.ruc.edu.cn/ermsClient/eresourceInfo.do?rid=136)


- 搜数标题："中国2003年各地区主要农产品出口贸易情况统计"、"中国2003年各地区主要农产品进口贸易情况统计"
- 可下载年限范围：2003-2013
- 数据来源：中国农业年鉴
- 数据频率：年度
- 文件格式：html-形式表格

- 搜数标题："中国2012年各地区农产品进出口总值统计"
- 数据来源：中国农业年鉴
- 可下载年限范围：2002-2012
- 数据频率：年度
- 文件格式：html-形式表格





# 3.数据抓取流程


## 3.1数据抓取策略

同时对两个数据源进行互补式抓取：

- 2014/6-2020/5：抓取中国海关总署，在线网址形式的html表格数据

- 2000/1-2014/5：抓取搜数网，下载好的html静态页面数据

对于中国海关总署网站数据，有两个抓取办法：

>
a. 理想方法：通过Rselenium+docker的办法自动化抓取到所需表格的所有月份页面的实际网址url，然后批量下载保存成静态页面。

>
b. 手工办法：进入海关网站，手动下载每个月份页面的实际静态网页到本地。工作量初步估计：进出口量值的页面数=7年x12月x2类=`r 7*12*2`


## 3.2抓取环节1：下载静态pdf

基本过程：

a. 中国农产品进出口月度统计报告。进入[主站](http://wms.mofcom.gov.cn/article/ztxx/ncpmy/ncpydtj/200603/20060301783733.shtml)。

b. 第一步是需要获得指定“主题”（例如“中国农产品进出口月度统计报告”）下所有月份的实际网址url。具体细节见后面的`R`代码（见节\@ref(auto-url)）。

c. 根据获取的实际url，下载全部静态表格网页到本地，并批量命名。

d. 通过下载所有月份的静态网页pdf（含有数据表），然后再进行数据整合。


主要关注如下pdf页面表格：

- 表12：2008年1月分地区出口情况

- 表13：2008年1月分地区进口情况

### 获得网站所有pdf的urls

**操作提示**：

- 2018年直接是.rar压缩包下载，全年月份pdf都有。


```{r}
# 0. load R pkgs

require("rvest")
require("xml2")
require("httr")
require("stringr")
require("tidyverse")
require("tidyselect")

# navigate the home page
url_report <-"http://wms.mofcom.gov.cn/article/ztxx/ncpmy/ncpydtj/200603/20060301783733.shtml"
doc <-  read_html(url_report, encoding = "utf-8")

# obtain past year home page: 2002-2019
css_past <- "#zoom> p > a, #zoom> p > span> a, #zoom> p > span> span>a"
### year title
title_past <- doc %>%
  html_nodes(css = css_past) %>%
  html_text()
### year urls
url_latest <- doc %>%
  html_nodes(css = css_past) %>%
  html_attr("href")
### table
nav_urls <- tibble(year=title_past, urls=url_latest) %>%
  filter(year!="2018年")    # filter year 2018 is rar zip file

# get pdfs' urls: the latest years  2020
css_pdf <- "#zoom > table > tbody > tr > td > p > a"

title_latest <- doc %>%
  html_nodes(css = css_pdf) %>%
  html_text()

url_latest <- doc %>%
  html_nodes(css = css_pdf) %>%
  html_attr("href")

tbl_out <- tibble(title=title_latest, urls=url_latest)

# get pdfs' urls: the past years 2002-2019 (not 2018)
n_year <- nrow(nav_urls)
i <- 2

# loop for all years homepage
### special css case in 2017 and 
css_pdf <- "#zoom > table > tbody > tr > td > p > a, #zoom > table > tbody > tr > td > p > span >a, #zoom > table > tbody > tr > td > a"
for (i in 1:n_year){
  # the home page
  doc_pdf <- read_html(nav_urls$urls[i])
  
  title_pdf <- doc_pdf %>%
    html_nodes(css = css_pdf) %>%
    html_text()
  
  url_pdf <- doc_pdf %>%
    html_nodes(css = css_pdf) %>%
    html_attr("href")
  
  tbl_pdf <- tibble(title=title_pdf, urls=url_pdf) %>%
    filter(title!="")   # special 2002-2004
  tbl_out <- bind_rows(tbl_out, tbl_pdf)
}


tbl_urls <- tbl_out %>%
  mutate(year= as.numeric(str_extract(title, "(\\d{4})(?=年)")),
         month_raw= str_extract(title, "(?<=年)(.+)(?=月)")) %>%
  mutate(month = as.numeric(month_raw),
         month = str_pad(month, width = 2, side="left", pad="0")) %>%
   # file names
  mutate(name_file = paste0(year, "-",month, ".pdf")) %>%
  add_column(ID=1:nrow(.), .before = "title") 
  


### backup for check
openxlsx::write.xlsx(tbl_urls, "tbl-urls-backup.xlsx", row.names = F)

```




### 下载pdf——使用系统浏览器

如果前述方法不能顺利执行，则可以尝试使用系统自带浏览器。（测试成功率较高）。


```{r}
# 0. load R pkgs
library("RSelenium")
library("xml2")

tbl_urls <- openxlsx::read.xlsx("tbl-urls-backup.xlsx", sheet=1)

path_pdf <- paste0("pdf/", tbl_urls$name_file)

for (i in 1:nrow(tbl_urls)){
  download.file(tbl_urls$urls[i], path_pdf[i])
  
  print(paste0("下载第", i, "个pdf文件：", tbl_urls$name_file[i] ))

}


```



### 核对最终下载页面数

（可能会少于后者）与实际下载页面数是否一致

因为原始`tbl_com`里可能存在标题重复或错误，从而导致最终下载页面数会少于实际下载页面数。

处理办法如下：
- 列表最终下载页面，`tbl_files`。
- 列表实际下载页面，`tbl_urls`。
- 查看二者行数是否一致（前者少于后者9个）
- 检查实际下载页面，`tbl_urls`的列`name_file`是否存在重复，以及哪些行重复
- 追踪到tbl_com.csv，定位到这些行，并查明原因。一类原因是title错误，确认后直接在csv里修改好；二类原因是确实重复了，那可以直接忽略（因为下载后会自动覆盖这些重复urls）

下面是检查代码：

```{r, eval=FALSE}
# read backup urls
tbl_full <- read.csv("tbl-urls-backup-custom.csv") %>%
  filter(number=="表15") %>%
  add_column(ID=1:nrow(.), .before = "number") %>%
  select(ID, file_name) %>%
  rename("name_file" = "file_name")
  


# files html path
### you should change the dir according to the purpose
### tbl15 or tbl16
files_dir <- here::here("data", "v8", "03-trade-custom", "04-tbl15-country-export","html-01-custom")
# files_dir <- here::here("data", "v8", "03-trade-custom", "05-tbl16-country-import","html-01-custom")

files_html <- list.files(str_c(files_dir,"/"))
page_url <- str_c(files_dir, files_html, sep = "/")

# table the files
tbl_files <- tibble(name_file=files_html,
                    size = round(file.size(page_url)/1024,0)) %>%
  add_column(ID = 1:nrow(.), .before = "name_file") %>%
  mutate(year= str_extract(name_file, "(\\d{4})(?=-)"),
         month= str_extract(name_file, "(?<=-)(\\d{2})")) %>%
  mutate(currency   = str_extract(name_file, "(.{1})(?=\\.)")) %>%
  mutate(path_file = paste0("html-01-custom/", name_file)) 

# check
check_sum <- tbl_files %>%
  group_by(year, month) %>%
  summarize(n=n(), .groups="drop")

# check file size not normal
check_size <- tbl_files %>%
  select(name_file, size) %>%
  filter(size < 200) 

check_error <- check_size %>%
  left_join(., tbl_full, by="name_file")

list_download <- check_error$ID


```


以下是**缺失性**核查的结果：

- **表15**：对部分国家(地区)出口商品类章金额表：
    - 2014-04 到2015-01，都是图片格式，无法识别。
    - 2017-08。图片格式，无法识别。


- **表16**：自部分国家(地区)进口商品类章金额表：
    - 2016-12，缺失网页表格；但是可以下载到excel表格。 后面需要单独处理。保存在`05-tbl16-country-import\hack-sooshu\extra-custom\2016-12-D.xls`和`05-tbl16-country-import\hack-sooshu\extra-custom\2016-12-R.xls`


## 3.3导出pdf为对应的csv文件

### 前期测试

```{r}
#install.packages("tabulizer")
library("tabulizer")

f <- system.file("2008-01.pdf", package = "tabulizer")
tab <- extract_tables(f, pages = 27)

extract_areas(file = pth, pages = 27, output = "data.frame")

agri_report <- extract_tables(file = "2008-01.pdf", pages = 27,
                              #area = list(c(6,2,25,19)),
                              guess = F,
                              encoding = "gb18030", 
                              method = "decide", output = "data.frame") %>%
  as_tibble()

Encoding("鍖椾含")

iconv("鍖椾含", from="utf-8", to="unicode")

guess_encoding(agri_report)
```


### 函数0：基础函数

```{r}
# function for count white spaces
countWhiteSpaces <- function(x) attr(gregexpr("(?<=[^#])[#]+(?=[^#])", x, perl = TRUE)[[1]], "match.length")

# function for seek the whitespace style
###install.packages("mgsub")
###library("mgsub")
str_seek <- function(strs, style=n_style,rep=n_rep){
  out <- mgsub::mgsub(strs, pattern = style, replacement =rep,perl = F)
  return(out)
}


# function for calculate vector modes
getModes <- function(x) {
  ux <- unique(x)
  tab <- tabulate(match(x, ux))
  ux[tab == max(tab)]
}
```

### 函数3：核验原始表格

```{r}
#dt <- rawpage
# check styles
check_tbl <- function(tbl){
  tbl_dt <- tbl %>%
  mutate(n=map(.x = value, .f = countWhiteSpaces)) %>%
  mutate(len = lengths(n), 
         min=map(.x=n, .f=min), 
         max=map(.x=n, .f=max), 
         mode=map(.x = n, .f = getModes))
  return(tbl_dt)
}

#check <- tbl_check(rawpage)

```


### 函数4：得到清洗后的表

```{r}
#tbl_check <- check
# obtain the table output
get_split <- function(tbl_check, style=n_style, rep =n_rep,vars_eng=names_eng){
  len_max <- tbl_check %>% .$len %>% unlist() %>% max()
  tbl_seek <- tbl_check %>%
    mutate(str= if_else(
      len==len_max,gsub("(#){1,50}", "&", value, perl = F), # for full len
      mgsub::mgsub(value, pattern = style, 
                   replacement =rep, perl = F) # for other
      ) ) %>%
    select(str) %>%
    #mutate(str= str_replace(str, "-", "NA")) %>%
    separate(str,into = vars_eng, sep = "&")  %>%
    mutate_at(all_of(vars_eng[-1]), .funs = as.numeric, digits=2)
}

```

### pdf抓取为csv

```{r}
# install.packages("pdftools")
library("pdftools")

# read pdf as text
pth<- "2008-01.pdf" 
txt <- pdf_text(pth)


# set pars for seek whitespace style
n_start <- c(61,51,41,1) # not zero
n_end <-  c(70,60,50,40)
n_style <- paste0("(#){", n_start, ",", n_end, "}")
n_rep <-  c("&&&&","&&&", "&&", "&" )




# extract the text page of pdf
txt_page<- txt[27]

# detect info section
detect_title <- str_extract(txt_page, "(表\\d{1,2}：.+情况)")
detect_unit <- str_extract(txt_page, "(金额单位：.{3})")
type <- str_extract(detect_title, ".{2}(?=情况)")
currency <- str_extract(detect_unit, "(?<=单位：)(.+)")

# obtain data section
txt_tbl <- gsub( "((.|\\n)*)(?=北京)","",txt_page, perl = T) %>%
  str_split(., "\r\n") %>%
  .[[1]] %>%
  as_tibble()%>%
  mutate(value=gsub(" ", "#", value))%>%
  mutate(value=gsub(",", "", value))

# detect the start and end row
row_first <-  which(str_detect(txt_tbl$value,"北京"))
row_last <-  which(str_detect(txt_tbl$value,"新疆"))
range <- row_first:row_last

# get the raw table
tbl_raw <- txt_tbl %>%
  # delete rows unnecessary
  .[range,] 

# check styles
check <- check_tbl(tbl= tbl_raw)

# create names
n_cols <- unique(check$len) 
names_eng <- c("V0", paste0("V",1:n_cols ))

# obtain the split table
tbl_split <- get_split(tbl_check = check) %>%
  select(all_of(names_eng))

# add columns
tbl_add <- tbl_split %>%
  add_column(trade=type, .before = "V0")

  
#str(tbl_split)
  



```



### 2.2.1.下载并保存主站网页源代码

注意：
>
- 具体操作：chrome浏览器打开网址 $\Rightarrow$ 查看源代码 $\Rightarrow$ 复制所有源代码到 notepad++ 保存为UTF-8格式下的txt文件。
>
- 说明：因为网站采用了js框架，不能直接静态拉取页面的所有信息（例如`read_html()`）。而采用动态拉取方法也会失效（例如`RSelenium + docker`的方式）。


### 2.2.2.自动抓取url的R代码实现{#auto-url}

我们可以直接分析网页的源代码文件^[动态化抓取的办法见[参考资料](https://statistics.berkeley.edu/computing/r-reading-webpages)]。

> **说明**："2015年进口主要商品量值表" 1月页面链接无效

以下为R代码具体实现过程：

```{r, echo=T}
# step 1: save the source code web as .txt file

# step 2: read the txt file with correct encoding type
the_page <- readLines("00-web-hack/page-all-index.txt", encoding = "UTF-8")

# step 3: specify the needed table title
the_title <- "自部分国家(地区)进口商品类章金额表"

# step 4: create the whole url scrape function

get_url <- function(page=the_page, title=the_title){
  # 4.1 locate the line number
  id_currency <- grep("表名 （", page)
  id_head <- grep(title, page)
  id_url <- id_head+1
  # 4.2 choose the useful lines
  tbl_choose <- tibble(id_currency = grep("表名 （", page),
         id_head = grep(title, page),
         id_url = id_head+1,
         txt_currency=page[id_currency],
         txt_head= page[id_head],
         txt_url=page[id_url])
  # 4.3 extract the currency/year/variables
  tbl_extract <- tbl_choose %>%
    # extract currency
    mutate(currency= str_extract_all(txt_currency, "(?<=（)(.+)(?=）)")) %>%
    mutate(currency= str_replace(currency, " ", "")) %>%
    # extract year
    mutate(year=as.numeric(str_extract_all(txt_head, "(?<=\\))(\\d{4})"))) %>%
    # extract variables
    mutate(variables=str_extract(txt_head, "进口|出口|进出口"))
  # 4.4 map function which we can extract the month and its real url
  extract_url <- function(txt){
    text_tbl <- tibble(text=unlist(str_split(txt,pattern ="</a>"))) %>%
      filter(str_detect(text,"月")) %>%
      mutate(text=str_replace_all(text,"><a", "<a")) %>%
      # extract month
      mutate(month=str_extract(text,  pattern = "(?<=>)(.+)(?=月)")) %>%
      # extract url
      mutate(url=str_extract(text,  pattern = "(?<=http://)(.*.html)"))
    return(text_tbl)
  }
  # 4.5 map and unnest  
  tbl_map <- tbl_extract %>%
    mutate(tbl=map(.x=txt_url, .f = extract_url )) %>%
    unnest(cols = tbl) %>%
    select(currency,variables,year,month, url ) 
    
  
  return(tbl_map)
}

# tbl_show <- get_url(title = "自部分国家(地区)进口商品类章金额表")

```


下面，我们把所有感兴趣的主体的url地址全部抓取下来：

```{r, echo=TRUE}
# set topic we need
all_title <- c("进出口商品类章总值表",  # 表4 
               "出口主要商品量值表",    # 表13
               "进口主要商品量值表",    # 表14
               "出口商品类章金额表",    # 表15
               "进口商品类章金额表")    # 表16
number_tbl <- paste0("表", c(4,13:16))
# loop for all table
### i <-1
tbl_all <- NULL
for (i in 1:length(all_title)){
  tbl_show <- get_url(page=the_page, title = all_title[i]) %>%
    add_column(table=all_title[i], .before = "currency") %>%
    add_column(number=number_tbl[i], .before = "table") 
  tbl_all <- bind_rows(tbl_all, tbl_show)
  print(number_tbl[i])
}

tbl_all <- tbl_all %>%
  # handle the month with leading zero
  mutate(month= str_replace(month, "1-2", "2")) %>%
  mutate(month=str_trim(month, side = "both")) %>%
  mutate(month=str_pad(month, width = 2, pad = "0"))

```

抓取的最后结果如下：

```{r}
# just for chunk set: screenshot.opts = list(selector = ".dataTable")
#install.packages("webshot")
#webshot::install_phantomjs()
require("webshot")
# total numbers of urls
n_tot <- tbl_all %>% nrow(.)
# show result
tbl_all %>%
    filter(!is.na(url)) %>%
    DT::datatable(caption=paste0("月度网页实际url自动抓取结果(条数n=", n_tot, ")"),
                  options = list(dom="tim", pageLength=10), rownames = FALSE)
```

最后我们把抓取结果保存在本地：

```{r, echo=TRUE, eval=FALSE}
# write out files
file_name<- paste0("00-web-hack/auto-url-",Sys.Date(),".csv")
write.csv(tbl_all, file = file_name, row.names = FALSE)
```

### 2.2.3. 批量化下载url静态页面到本地

**异常页面**：

- 图片形式的页面。2017-08-D.html


**第一步**：读取并整理所有url。

```{r, echo=T, eval=FALSE}
# set topic we need
all_title <- c("进出口商品类章总值表",  # 表4 
               "出口主要商品量值表",    # 表13
               "进口主要商品量值表",    # 表14
               "出口商品类章金额表",    # 表15
               "进口商品类章金额表")    # 表16
number_tbl <- paste0("表", c(4,13:16))

# read the csv file on all urls
tbl_all <- read.csv("00-web-hack/auto-url-2020-07-13.csv")

title_match <- tibble(file_dir = c("01-tbl04-cat", 
  "02-tbl13-cat-export","03-tbl14-cat-import",
  "04-tbl15-country-export","05-tbl16-country-import"),
  table = all_title)

#  tidy url and create file path for each url
tbl_urls <- tbl_all %>% 
  dplyr::filter(!is.na(url)) %>%          # filter NULL urls
  #dplyr::filter(currency=="人民币") %>%   # only need the RMB table
  mutate(cur = if_else(currency=="人民币", "R", "D")) %>%
  mutate(url=paste0("http://", url)) %>%  # fill the actual urls
  # for the name with leading zero
  mutate(month=str_pad(month, width = 2, pad = "0")) %>%  
  # the file name
  mutate(file_name = paste0(year, "-", month ,"-",cur,".html")) %>%
  # the dir name
  left_join(x=., y=title_match, by="table") %>%
  # the file path
  mutate(file_path = paste0(file_dir, "/html-01-custom/", file_name))

# backup for reuse
write.csv(tbl_urls, "tbl-urls-backup-custom.csv", row.names = F)
```



**第二步**：利用docker+RSelenium下载所有url对应的静态html到本地。

- 环境准备。安装docker desktop，进行配置（主要是registry mirror设置为国内服务器；确保bios开启虚拟化服务）。拉取pull安装selenium/chrome以及firefox到本地。

- 设计Rselenium包的server-client交互过程。docker启用container时，浏览器的选择很重要，chrome无法进入网页，但是firefox可以。

- 利用其他R包配合，自动化下载静态html到本地硬盘。其中的关键就是利用RSelenium读取源文件，然后用`xml2::read_html()`转化为html，最后再由`xml2::write_xml()`写出到本地。

说明：

- “表14：2015年进口主要商品量值表（人民币）1月份”的url为空。

具体R代码如下：

#### 使用selenium浏览器下载

可以尝试使用docker selenium浏览器。（能否成功，根据目标网站情况而定）。

```{r, eval=FALSE,echo=T}
# 0. load R pkgs
library("RSelenium")
library("xml2")

# you should set the table number to download all html for each url
url_list <- tbl_urls %>%
  filter(number == "表15")  

# 1. run docker service and container
#### you should run and start docker desktop first.
#### then run code in 'window power shell': docker run -d -p 4445:4444 selenium/standalone-firefox

# 2. create the remote driver
remDr <- remoteDriver(remoteServerAddr = "192.168.56.1", port = 4445L, 
                      browserName = "firefox")
## open the connect
remDr$open()

# i<-  c(75)

for (i in 75:nrow(url_list) ){
  # navigate the url
  remDr$navigate(url_list$url[i])
  # wait seconds
  Sys.sleep(15)
  # confirm you got there
   print(remDr$getTitle())
  # check on there
  remDr$screenshot(display = TRUE)
  # get the page source
  webpage <- remDr$getPageSource()[[1]]
  # wait seconds
  Sys.sleep(2)
  # read as html object
  html_obj <- xml2::read_html(webpage)
  # write out the html file
  xml2::write_xml(html_obj, file=url_list$file_path[i])
  
  print(paste0("第",i,"个页面。",url_list$file_name[i], "已经下载完成！"))
}


```

#### 使用系统浏览器下载

如果前述方法不能顺利执行，则可以尝试使用系统自带浏览器。（测试成功率较高）。

```{r}
# 0. load R pkgs
library("RSelenium")
library("xml2")

# you should set the table number to download all html for each url
url_list <- tbl_urls %>%
  filter(number == "表16")  

# 1. run docker service and container
#### you should run and start docker desktop first.
#### then run code in 'window power shell': docker run -d -p 4445:4444 selenium/standalone-firefox

driver <- rsDriver(browser=c("firefox"))
remDr <- driver[["client"]]
remDr$open()

# i<-  c(1)

for (i in 4:nrow(url_list) ){
  # navigate the url
  remDr$navigate(url_list$url[i])
  # wait seconds
  Sys.sleep(3)
  # confirm you got there
   print(remDr$getTitle())
  # check on there
  #remDr$screenshot(display = TRUE)
  # get the page source
  webpage <- remDr$getPageSource()[[1]]
  # wait seconds
  Sys.sleep(1)
  # read as html object
  html_obj <- xml2::read_html(webpage)
  # write out the html file
  xml2::write_xml(html_obj, file=url_list$file_path[i])
  
  print(paste0("第",i,"个页面。",url_list$file_name[i], "已经下载完成！"))
}



#close the driver
remDr$close()

#close the server
remDr$server$stop()
```



#### 核对最终下载页面数

（可能会少于后者）与实际下载页面数是否一致

因为原始`tbl_com`里可能存在标题重复或错误，从而导致最终下载页面数会少于实际下载页面数。

处理办法如下：
- 列表最终下载页面，`tbl_files`。
- 列表实际下载页面，`tbl_urls`。
- 查看二者行数是否一致（前者少于后者9个）
- 检查实际下载页面，`tbl_urls`的列`name_file`是否存在重复，以及哪些行重复
- 追踪到tbl_com.csv，定位到这些行，并查明原因。一类原因是title错误，确认后直接在csv里修改好；二类原因是确实重复了，那可以直接忽略（因为下载后会自动覆盖这些重复urls）

下面是检查代码：

```{r, eval=FALSE}
# read backup urls
tbl_full <- read.csv("tbl-urls-backup-custom.csv") %>%
  filter(number=="表15") %>%
  add_column(ID=1:nrow(.), .before = "number") %>%
  select(ID, file_name) %>%
  rename("name_file" = "file_name")
  


# files html path
### you should change the dir according to the purpose
### tbl15 or tbl16
files_dir <- here::here("data", "v8", "03-trade-custom", "04-tbl15-country-export","html-01-custom")
# files_dir <- here::here("data", "v8", "03-trade-custom", "05-tbl16-country-import","html-01-custom")

files_html <- list.files(str_c(files_dir,"/"))
page_url <- str_c(files_dir, files_html, sep = "/")

# table the files
tbl_files <- tibble(name_file=files_html,
                    size = round(file.size(page_url)/1024,0)) %>%
  add_column(ID = 1:nrow(.), .before = "name_file") %>%
  mutate(year= str_extract(name_file, "(\\d{4})(?=-)"),
         month= str_extract(name_file, "(?<=-)(\\d{2})")) %>%
  mutate(currency   = str_extract(name_file, "(.{1})(?=\\.)")) %>%
  mutate(path_file = paste0("html-01-custom/", name_file)) 

# check
check_sum <- tbl_files %>%
  group_by(year, month) %>%
  summarize(n=n(), .groups="drop")

# check file size not normal
check_size <- tbl_files %>%
  select(name_file, size) %>%
  filter(size < 200) 

check_error <- check_size %>%
  left_join(., tbl_full, by="name_file")

list_download <- check_error$ID


```


以下是**缺失性**核查的结果：

- **表15**：对部分国家(地区)出口商品类章金额表：
    - 2014-04 到2015-01，都是图片格式，无法识别。
    - 2017-08。图片格式，无法识别。


- **表16**：自部分国家(地区)进口商品类章金额表：
    - 2016-12，缺失网页表格；但是可以下载到excel表格。 后面需要单独处理。保存在`05-tbl16-country-import\hack-sooshu\extra-custom\2016-12-D.xls`和`05-tbl16-country-import\hack-sooshu\extra-custom\2016-12-R.xls`



### 2.2.4. 把html转换为对应的csv，并提取整合有效数据

具体的R操作在每一类表（表4、表13、……）的文件夹里【也可见视频 step3】。

**第一步**：利用R编程，自动把本地的静态html文件，分别转换为csv文件（xlsx就能打开查看的。）。

```{r, fig.cap="把所有html文件分别转换成对应的csv文件"}
knitr::include_graphics("pic/01-html-to-csv.png")
```



**第二步**：从所有的csv文件中提取有用的信息。合并并保存到本地csv。

```{r, fig.cap="提取csv文件中的有用数据，并整合成一个数据文件"}
knitr::include_graphics("pic/02-extract-needed.png")
```
