---
title: "中国商务部农产品贸易专题网站抓取"
author: "胡华平"
date: "`r Sys.Date()`"
output:
  bookdown::word_document2:
    fig_caption: yes
    toc: yes
    toc_depth: 4
    reference_docx: report-reference.docx
  word_document:
    toc: no
    toc_depth: '4'
  bookdown::html_document2:
    number_sections: yes
    toc: no
    fig_caption: yes
    toc_float: yes
always_allow_html: yes
---

```{r, echo=FALSE}
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE,
                      fig.align='center',fig.width=10, fig.height=7) # Places figures on their own pages
options(
  htmltools.dir.version = FALSE, 
  formatR.indent = 2, width = 55, 
  digits = 2,scipen=999,tinytex.verbose = TRUE,
  knitr.kable.NA = '',
  fig.width=12, fig.height=8)

```

```{r}
require("rvest")
require("stringr")
require("tidyverse")
require("tidyselect")
```

\newpage

# 1.研究目标

目标是把中国商务部发布的《中国农产品进出口月度统计报告》中的——(31个省区)分地区进/出口情况——所有数据整理出来。初步分析，我国各省区农产品进/出口的趋势和关系。



# 2.数据集

## 2.1资料来源1：商务部网站

中国商务部-[农产品贸易专题网站](http://wms.mofcom.gov.cn/article/ztxx/ncpmy/)

**《中国农产品进出口月度统计报告》**:

- 文件标题："中国农产品进出口月度统计报告2020年1月"
- 可下载年限范围：2002/01-2020/02
- 数据频率：月度
- 文件格式：pdf-制式表格

**《重要农产品出口月度报告》**:

- 文件标题：
    - ["中国农产品出口月度统计报告-大米"](http://wms.mofcom.gov.cn/aarticle/Nocategory/200609/20060903269241.html)
    - ["中国农产品出口月度统计报告-小麦"](http://wms.mofcom.gov.cn/aarticle/Nocategory/200609/20060903269081.html)
    - [中国农产品出口月度统计报告-玉米](http://wms.mofcom.gov.cn/aarticle/Nocategory/200609/20060903269063.html)
- 可下载年限范围：2006/12-2020/03
- 数据频率：月度
- 文件格式：pdf-制式表格


## 2.2资料来源2：搜数网-中国农业统计年鉴


搜数网[人大权限](https://libproxy.ruc.edu.cn/ermsClient/eresourceInfo.do?rid=136)


- 搜数标题："中国2003年各地区主要农产品出口贸易情况统计"、"中国2003年各地区主要农产品进口贸易情况统计"
- 可下载年限范围：2003-2013
- 数据来源：中国农业年鉴
- 数据频率：年度
- 文件格式：html-形式表格

- 搜数标题："中国2012年各地区农产品进出口总值统计"
- 数据来源：中国农业年鉴
- 可下载年限范围：2002-2012
- 数据频率：年度
- 文件格式：html-形式表格



# 3.数据抓取流程


## 3.1数据抓取策略

同时对两个数据源进行互补式抓取：

- 2014/6-2020/5：抓取中国海关总署，在线网址形式的html表格数据

- 2000/1-2014/5：抓取搜数网，下载好的html静态页面数据

对于中国海关总署网站数据，有两个抓取办法：

>
a. 理想方法：通过Rselenium+docker的办法自动化抓取到所需表格的所有月份页面的实际网址url，然后批量下载保存成静态页面。

>
b. 手工办法：进入海关网站，手动下载每个月份页面的实际静态网页到本地。工作量初步估计：进出口量值的页面数=7年x12月x2类=`r 7*12*2`


## 3.2下载静态pdf

基本过程：

a. 中国农产品进出口月度统计报告。进入[主站](http://wms.mofcom.gov.cn/article/ztxx/ncpmy/ncpydtj/200603/20060301783733.shtml)。

b. 第一步是需要获得指定“主题”（例如“中国农产品进出口月度统计报告”）下所有月份的实际网址url。具体细节见后面的`R`代码（见节\@ref(auto-url)）。

c. 根据获取的实际url，下载全部静态表格网页到本地，并批量命名。

d. 通过下载所有月份的静态网页pdf（含有数据表），然后再进行数据整合。


主要关注如下pdf页面表格：

- 表12：2008年1月分地区出口情况

- 表13：2008年1月分地区进口情况

### 获得网站所有pdf的urls

**操作提示**：

- 2018年直接是.rar压缩包下载，全年月份pdf都有。


```{r}
# 0. load R pkgs

require("rvest")
require("xml2")
require("httr")
require("stringr")
require("tidyverse")
require("tidyselect")

# navigate the home page
url_report <-"http://wms.mofcom.gov.cn/article/ztxx/ncpmy/ncpydtj/200603/20060301783733.shtml"
doc <-  read_html(url_report, encoding = "utf-8")

# obtain past year home page: 2002-2019
css_past <- "#zoom> p > a, #zoom> p > span> a, #zoom> p > span> span>a"
### year title
title_past <- doc %>%
  html_nodes(css = css_past) %>%
  html_text()
### year urls
url_latest <- doc %>%
  html_nodes(css = css_past) %>%
  html_attr("href")
### table
nav_urls <- tibble(year=title_past, urls=url_latest) %>%
  filter(year!="2018年")    # filter year 2018 is rar zip file

# get pdfs' urls: the latest years  2020
css_pdf <- "#zoom > table > tbody > tr > td > p > a"

title_latest <- doc %>%
  html_nodes(css = css_pdf) %>%
  html_text()

url_latest <- doc %>%
  html_nodes(css = css_pdf) %>%
  html_attr("href")

tbl_out <- tibble(title=title_latest, urls=url_latest)

# get pdfs' urls: the past years 2002-2019 (not 2018)
n_year <- nrow(nav_urls)
i <- 2

# loop for all years homepage
### special css case in 2017 and 
css_pdf <- "#zoom > table > tbody > tr > td > p > a, #zoom > table > tbody > tr > td > p > span >a, #zoom > table > tbody > tr > td > a"
for (i in 1:n_year){
  # the home page
  doc_pdf <- read_html(nav_urls$urls[i])
  
  title_pdf <- doc_pdf %>%
    html_nodes(css = css_pdf) %>%
    html_text()
  
  url_pdf <- doc_pdf %>%
    html_nodes(css = css_pdf) %>%
    html_attr("href")
  
  tbl_pdf <- tibble(title=title_pdf, urls=url_pdf) %>%
    filter(title!="")   # special 2002-2004
  tbl_out <- bind_rows(tbl_out, tbl_pdf)
}


tbl_urls <- tbl_out %>%
  mutate(year= as.numeric(str_extract(title, "(\\d{4})(?=年)")),
         month_raw= str_extract(title, "(?<=年)(.+)(?=月)")) %>%
  mutate(month = as.numeric(month_raw),
         month = str_pad(month, width = 2, side="left", pad="0")) %>%
   # file names
  mutate(name_file = paste0(year, "-",month, ".pdf")) %>%
  add_column(ID=1:nrow(.), .before = "title") 
  


### backup for check
openxlsx::write.xlsx(tbl_urls, "tbl-urls-backup.xlsx", row.names = F)

```




### 下载全部pdf

如果前述方法不能顺利执行，则可以尝试使用系统自带浏览器。（测试成功率较高）。


```{r}
# 0. load R pkgs
library("RSelenium")
library("xml2")

tbl_urls <- openxlsx::read.xlsx("tbl-urls-backup.xlsx", sheet=1)

path_pdf <- paste0("pdf/", tbl_urls$name_file)

for (i in 11:nrow(tbl_urls)){
  download.file(tbl_urls$urls[i], path_pdf[i],mode = "wb" )
  
  print(paste0("下载第", i, "个pdf文件：", tbl_urls$name_file[i] ))

}


```



### 核对最终下载页面数

（可能会少于后者）与实际下载页面数是否一致

因为原始`tbl_com`里可能存在标题重复或错误，从而导致最终下载页面数会少于实际下载页面数。

处理办法如下：
- 列表最终下载页面，`tbl_files`。
- 列表实际下载页面，`tbl_urls`。
- 查看二者行数是否一致（前者少于后者9个）
- 检查实际下载页面，`tbl_urls`的列`name_file`是否存在重复，以及哪些行重复
- 追踪到tbl_com.csv，定位到这些行，并查明原因。一类原因是title错误，确认后直接在csv里修改好；二类原因是确实重复了，那可以直接忽略（因为下载后会自动覆盖这些重复urls）

下面是检查代码：

```{r, eval=FALSE}
# read backup urls
tbl_full <- openxlsx::read.xlsx("tbl-urls-backup.xlsx",colNames=T) 

# files html path
### you should change the dir according to the purpose
### tbl15 or tbl16
files_dir <- here::here("data", "v8", "04-trade-mofcom", "01-trade-region","pdf")
# files_dir <- here::here("data", "v8", "03-trade-custom", "05-tbl16-country-import","html-01-custom")

files_html <- list.files(str_c(files_dir,"/"))
page_url <- str_c(files_dir, files_html, sep = "/")

# table the files
tbl_files <- tibble(name_file=files_html,
                    size = round(file.size(page_url)/1024,0)) %>%
  add_column(ID = 1:nrow(.), .before = "name_file") %>%
  mutate(year= str_extract(name_file, "(\\d{4})(?=-)"),
         month= str_extract(name_file, "(?<=-)(\\d{2})")) %>%
  mutate(path_file = paste0("pdf/", name_file)) 

# check
check_sum <- tbl_files %>%
  group_by(year) %>%
  summarize(n=n(), .groups="drop")

# check file size not normal
check_size <- tbl_files %>%
  select(name_file, size) %>%
  filter(size < 200) 


```


以下是**缺失性**核查的结果：

- **表15**：对部分国家(地区)出口商品类章金额表：
    - 2014-04 到2015-01，都是图片格式，无法识别。
    - 2017-08。图片格式，无法识别。


- **表16**：自部分国家(地区)进口商品类章金额表：
    - 2016-12，缺失网页表格；但是可以下载到excel表格。 后面需要单独处理。保存在`05-tbl16-country-import\hack-sooshu\extra-custom\2016-12-D.xls`和`05-tbl16-country-import\hack-sooshu\extra-custom\2016-12-R.xls`


## 3.3导出pdf为对应的csv文件

### 前期测试

```{r}
#install.packages("tabulizer")
library("tabulizer")

f <- system.file("2008-01.pdf", package = "tabulizer")
tab <- extract_tables(f, pages = 27)

extract_areas(file = pth, pages = 27, output = "data.frame")

agri_report <- extract_tables(file = "2008-01.pdf", pages = 27,
                              #area = list(c(6,2,25,19)),
                              guess = F,
                              encoding = "gb18030", 
                              method = "decide", output = "data.frame") %>%
  as_tibble()

Encoding("鍖椾含")

iconv("鍖椾含", from="utf-8", to="unicode")

guess_encoding(agri_report)
```


### 函数0：基础函数

```{r}
# function for count white spaces
countWhiteSpaces <- function(x) attr(gregexpr("(?<=[^#])[#]+(?=[^#])", x, perl = TRUE)[[1]], "match.length")

# function for seek the whitespace style
###install.packages("mgsub")
###library("mgsub")
str_seek <- function(strs, style=n_style,rep=n_rep){
  out <- mgsub::mgsub(strs, pattern = style, replacement =rep,perl = F)
  return(out)
}


# function for calculate vector modes
getModes <- function(x) {
  ux <- unique(x)
  tab <- tabulate(match(x, ux))
  ux[tab == max(tab)]
}
```

### 函数3：核验原始表格

```{r}
#dt <- rawpage
# check styles
check_tbl <- function(tbl){
  tbl_dt <- tbl %>%
  mutate(n=map(.x = value, .f = countWhiteSpaces)) %>%
  mutate(len = lengths(n), 
         min=map(.x=n, .f=min), 
         max=map(.x=n, .f=max), 
         mode=map(.x = n, .f = getModes))
  return(tbl_dt)
}

#check <- tbl_check(rawpage)

```


### 函数4：得到清洗后的表

```{r}
#tbl_check <- check
# obtain the table output
get_split <- function(tbl_check, style=n_style, rep =n_rep,vars_eng=names_eng){
  len_max <- tbl_check %>% .$len %>% unlist() %>% max()
  tbl_seek <- tbl_check %>%
    mutate(str= if_else(
      len==len_max,gsub("(#){1,50}", "&", value, perl = F), # for full len
      mgsub::mgsub(value, pattern = style, 
                   replacement =rep, perl = F) # for other
      ) ) %>%
    select(str) %>%
    #mutate(str= str_replace(str, "-", "NA")) %>%
    separate(str,into = vars_eng, sep = "&")  %>%
    mutate_at(all_of(vars_eng[-1]), .funs = as.numeric, digits=2)
}

```

### pdf抓取为csv

#### 异常提示

2002/01-2003/02：一张表跨两页。只有出口。**解决办法**：已处理好。
-   2002/01和2002/02，没有上年同期金额。

2003-03~2004-12：省份异常，安徽开头，重庆结尾。**解决办法**：已处理好。

2005-01~2005-12：省份异常，上海开头，黑龙江结尾。**解决办法**：已处理好。
- 2005-02 pdf格式异常，无法读取。**解决办法**：手动处理

2006-01：省份排序异常。上海开头，黑龙江结尾。**解决办法**：已处理好。

2016-09: 进出口表被整合在一块，没法识别。**解决办法**：手动处理/上年同期。


#### 抓取代码

```{r}
# install.packages("pdftools")
library("pdftools")

# read backup urls
tbl_urls <- openxlsx::read.xlsx("tbl-urls-backup.xlsx",colNames=T) 

# files pdf path
### you should change the dir according to the purpose
files_dir <- here::here("data", "v8", "04-trade-mofcom", "01-trade-region","pdf")


files_html <- list.files(str_c(files_dir,"/"))
page_url <- str_c(files_dir, files_html, sep = "/")

# table the files
tbl_files <- tibble(name_file=files_html,
                    size = round(file.size(page_url)/1024,0)) %>%
  add_column(ID = 1:nrow(.), .before = "name_file") %>%
  mutate(year= str_extract(name_file, "(\\d{4})(?=-)"),
         month= str_extract(name_file, "(?<=-)(\\d{2})")) %>%
  mutate(month_num= as.numeric(month)) %>%
  mutate(path_file = paste0("pdf/", name_file)) 


# set pars for seek whitespace style
n_start <- c(61,51,26,1) # not zero
n_end <-  c(70,60,50,25)
n_style <- paste0("(#){", n_start, ",", n_end, "}")
n_rep <-  c("&&&&","&&&", "&&", "&" )

list_province <- c("北京","天津","河北","山西","内蒙古","辽宁","吉林","黑龙江","上海","江苏","浙江","安徽","福建","江西","山东","河南","湖北","湖南","广东","广西","海南","重庆","四川","贵州","云南","西藏","陕西","甘肃","青海","宁夏","新疆")

i <- 2002
j <- 2
k <- 1

# loop years
for (i in 2020:2020) {
  # filter year and month
  tbl_ym <- tbl_files %>%
    filter(year==i)

  i_len <- nrow(tbl_ym)
  # loop months
  for (j in c(1:3)) {
    # read pdf as text
    pth<- tbl_ym$path_file[j]
    txt <- suppressWarnings(suppressMessages(pdf_text(pth)))
    
    
    # focus the pages
    page_target <- which(str_detect(txt, "北京|新疆"))
    page_raw <- txt[page_target]
    if (length(page_raw)==0) stop("pdf文件读取出错，请核实！")
    
    # loop province
    #k <- 22
    tbl_province <- NULL
    for (k in 1:length(list_province)){
      # detect the page which contain certain province
      page_province <- which(str_detect(txt, list_province[k]))
      # skip iteration and go to next iteration：k=22 in 2002-2
      if (length(page_province)==0) {
        print(paste0(list_province[k], "不在pdf表格中，请核实！"))
        next
        }
    
      # pattern to obtain data
      pattern_province <- paste0("(",list_province[k], ".*)(?=\r\n)")
      
      dt_province <- str_extract_all(txt[page_target], 
                                     pattern_province) %>%
        unlist() %>%
        as_tibble() %>%
        mutate(value=gsub(" ", "#", value))%>%
        mutate(value=gsub(",", "", value)) %>%
        mutate(value=gsub("%", "", value))
      
      check <- check_tbl(dt_province)
      
      # create names
      n_cols <- max(check$len)
      names_eng <- c("V0", paste0("V",1:n_cols ))
      
      
      tbl_split <- get_split(check,vars_eng = names_eng) %>%
        add_column(page=page_province)
      
      tbl_province <- bind_rows(tbl_province, tbl_split)
      print(paste0("第",k,"个省：",list_province[k]))
    }
    
    # obtain currency unit:
      detect_unit <- str_extract(page_raw, "(单位.+)|(金额.+)")[1]
      
      currency <- str_extract_all(detect_unit, "(?<=：|单位)(.+)") %>%
        str_trim(side = "both") %>%
        str_replace("：", "")
      if (length(currency)==0) stop("金额单位无法识别，请核实！")
      tbl_unit <- tibble(page=page_target, currency= currency )
      
    
      # obtain trade type: export or import
      title_target <- which(str_detect(txt, "表.+分地区"))
      txt_page <- txt[title_target]
      detect_title <- str_extract(txt_page, "(表.+分地区.{2}情况)")
      type <- str_extract(detect_title, ".{2}(?=情况)")
      if (length(page_raw)==2 & (page_target[1]!=page_target[2])){
        tbl_title<- tibble(page=page_target, type=type)
      } else {
        tbl_title<- tibble(page=title_target, type=type)
      }
    
      tbl_out <- tbl_province %>%
        left_join(., tbl_unit, by = "page") %>%
        left_join(., tbl_title, by = "page") %>%
          add_column(year= i, .before = "V0") %>%
          add_column(month= str_pad(j, width = 2, pad = "0"), 
                     .before = "V0") %>%
        select(year, month, type, currency, everything()) %>%
        mutate(V0=str_replace(V0, "省|市|自治区", ""),
               V0= str_replace(V0,"壮族|回族|维吾尔", "")) %>%
        mutate(V0= factor(V0, levels=list_province)) %>%
        arrange(type, V0)
  
   # check actual rows and theory rows
    rows_act <- nrow(tbl_out) 
    
    if (length(title_target) == 1){
       rows_theory <- 31
    } else if (length(title_target) == 2){
       rows_theory <- 31*2        # two table
    }
    
    
  if (rows_theory == rows_act) {
    print(paste0(i,"年", j, "月。恭喜！实际数据点数与理论一致！数据点数=", rows_act))
  } else {
    print(paste0(i,"年", j, "月。不妙！实际数据点数与理论不一致！实际数=", rows_act, ";理论数=", rows_theory))
  }
  
  # checking when there were inconsistent
  ttt <- tbl_out %>%
    group_by(type) %>%
    summarize(n=n(), .groups="drop")
  
  # wait to see the result
  Sys.sleep(1)
  
 if (rows_theory != rows_act) warning("请检查，数据量与预期不一致！")
  
  # files csv path
  path_csv <- paste0("csv-mofcom/",i, "-",str_pad(j, width = 2, pad = "0"), ".csv")
  write.csv(tbl_out, path_csv, row.names = F)

  }
}

```

## 3.4合并全部csv表格有效数据

### 异常提示

2002/01：贸易类型没有识别出来。**解决办法**：已处理好。

### 合并代码

```{r, eval=FALSE, echo=FALSE}
# set for chinese header
vars_chn <- c("年度","月份", "序号","类章序号","类章名目" ,
             "出口_当月", "出口_累计","进口_当月", "进口_累计",
             "累计同期变动_出口","累计同期变动_进口")
vars_eng <- c("Year","Month", "ID",
             "cat", paste0("X", 1:7))
cat_list <- paste0(str_pad(c(1:4,7:8,10:11,15,31,51:52), width = 2, pad = "0"),
                   "章")

# files path
csv_dir <- here::here("data", "v8", "04-trade-mofcom","01-trade-region", "csv-mofcom")
csv_files <- list.files(str_c(csv_dir,"/"))
csv_url <- str_c(csv_dir, csv_files, sep = "/")


# i <-2 length(csv_files)
tbl_out <- NULL
for (i in 1:length(csv_files)) {
  path_csv <- csv_files[i]
  
  # table for checking with the csv file
  tbl_tem <- read.csv(csv_url[i], header = T)  
  
  # check
  if(is.na(tbl_tem$V0)) stop("省份识别有问题，请核实！")
  if(is.na(tbl_tem$currency)) stop("单位识别有问题，请核实！")
  
  # add columns if needed
  n_V<- sum(str_detect(names(tbl_tem), "V\\d{1}"))
  if (n_V==3){
    tbl_tidy <- tbl_tem %>%
      rename_at(vars("V2"),~"V3") %>%
      add_column(V2="", .after = "V1") %>%
      mutate(V2=as.numeric(V2))
  } else {
    tbl_tidy <- tbl_tem 
  }
  
  # consistency of the unit and value
  tbl_value <- tbl_tidy %>%
    mutate(V1=ifelse(currency=="美元", V1/10000, V1),
           V2=ifelse(currency=="美元", V2/10000, V2)) %>%
    mutate(currency=ifelse(currency=="美元", "万美元", currency))
  
  # row bind 
  tbl_out <- bind_rows(tbl_out, tbl_value)
  print(csv_files[i])
}

# checking when there were inconsistent
ttt <- tbl_out %>%
  group_by(V0) %>%
  summarize(n=n())

```



## 3.5写出有效数据

```{r}
path_out <- str_c("tbl-mofcom-agri-trade-region-", Sys.Date(),".csv")
write.csv(tbl_out, path_out, row.names = F)
```